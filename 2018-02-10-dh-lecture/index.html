<!doctype html>
<html lang="de">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>DH Lecture: Topic Modeling (10 February 2018)</title>

		<link rel="stylesheet" href="../reveal/css/reveal.css">
		<link rel="stylesheet" href="../reveal/css/theme/simple.css">

      <!-- adjustments for serif.css -->
      <link rel="stylesheet" href="custom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="../reveal/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../reveal/css/print/pdf.css' : '../reveal/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			    <section data-markdown="" data-separator="^\n---\n" data-separator-vertical="^\n--\n" data-charset="utf-8">
<script type="text/template">

### –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã<br />–≤ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã—Ö –Ω–∞—É–∫–∞—Ö

<br /><br /><br />üìö &nbsp; &nbsp; üéº &nbsp; &nbsp; üó∫ &nbsp; &nbsp; <font style="font-size:5rem;width:100%;">üíª</font> &nbsp; &nbsp; üèõ &nbsp; &nbsp; üé® &nbsp; &nbsp; üéÆ

<br /><br /><br /><br /><br >
(40 –ª–µ–∫—Ü, 40 —Å–µ–º)<!-- .element: style="font-size:0.65em;" -->

--

## Fifth Lecture
### Distant Reading, Pt.¬†III: Topic Modeling

<br /><br /><br />
Frank Fischer &nbsp;¬∑&nbsp; Danil Skorinkin

ffischer@hse.ru &nbsp;¬∑&nbsp; dskorinkin@hse.ru<!-- .element: style="font-size:0.75em;" -->

<br /><br /><br />10 February 2018</p>

--

### Next Saturday: First Colloquium (1/2)

<br />
- groups of 3 to 4 prepare a 10-minute presentation using one of the following methods:
  - corpora studies
  - stylometry
  - topic modeling

--

### Next Saturday: First Colloquium (2/2)

<br />
- tasks can be divided like this:
  1. finding or assembling a corpus (collection of texts in *.txt format)
  2. loading the corpus into stylo or mallet and toying around with paramters to investigate the corpus
  3. save results (numbers, tables, visualisations) and produce a presentation

--

### Addition to Last Week (Stylometry)

<br />
- Roland Barthes: ["The Death of the Author"](https://ru.wikipedia.org/wiki/–°–º–µ—Ä—Ç—å_–∞–≤—Ç–æ—Ä–∞) (1967)
- essay (followed by an extensive discourse) according to which it is not important who wrote a text
- the idea behind it was to fade out biographical contexts and authorial intentions as basis for interpretation
- but: our stylometric experiments showed that there is something like an authorial signal in the text and that it can be measured

--

### Agenda for Today

<br />
1. Topic Modeling: Definition
2. LDA
3. Example (Goethe and Pushkin eat a pumpkin)
4. A More Realistic Example
5. Visualisation of Topic Models
6. Seminar: Hands-On with MALLET
7. Weekend üå¥

--

# ‚Üí

---

## Prelude with Wordle

- software for generating word clouds: [wordle.net](http://www.wordle.net/)
- example from Wikimedia Commons ([first chapter of "Don Quijote"](https://commons.wikimedia.org/wiki/File:Nube_de_etiquetas_-_Don_Quijote_de_la_Mancha.png)):

![Wordle example](images/wordle_don_quijote.jpg)<!-- .element width="600px;" -->

--

### Topic Modeling and the Digital Humanities

<br />"So many scholars in humanities departments are turning to [topic modeling] in their research that it is sometimes described as **part of the digital humanities in itself**."

<br />
<small>Benjamin M. Schmidt: [Words Alone: Dismantling Topic Models in the Humanities.](http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/)<br />In: Journal of Digital Humanities 2.1 (2012).</small>

--

## What Is a "Topic Model"?

<br />
"In machine learning and natural language processing, a **topic model** is a type of statistical model for discovering the **abstract 'topics'** that occur in a **collection of documents**. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body." ([Wikipedia](https://en.wikipedia.org/wiki/Topic_model))

--

## "Topic models are ‚Ä¶

<br />
‚Ä¶ the mother of all collocation tools."

<br />
<small>Matthew Jockers: Macroanalysis (2013), p. 123</small>

--

## "Topic modeling is ‚Ä¶

<br />
‚Ä¶ based on the idea that individual documents are made up of **one or more topics**. It uses emerging technologies in computer science to automatically cluster **topically similar documents** by determining the **groups of words** that tend to **co-occur** in them. Most importantly, topic modeling creates topical categories **without a priori subject definitions**."

<br />
<small>Sharon Block: [Doing More with Digitization](http://www.common-place-archives.org/vol-06/no-02/tales/) (2006)</small>

--

# ‚Üí

---

### The Emergence of Topic Modeling:<br />Latent Dirichlet Allocation (LDA)

- most popular topic model in use, introduced by David M. Blei et al. (2003)
- highly influential paper ([PDF here](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)), Google Scholar counts 21,703 citations to date
- LDA = "a generative probabilistic model for collections of discrete data such as text corpora"
- builds on the [Dirichlet distribution](https://ru.wikipedia.org/wiki/–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ_–î–∏—Ä–∏—Ö–ª–µ)
- especially useful for automated text classification in larger corpora and for finding hitherto unknown texts with similar topics

--

## LDA: How It Works

<br />
- general assumption: each document in the corpus features a set of themes (latent topics), which can be extracted using LDA
- the researcher has to toy around with model parameters:
  - number of topics has to be determined beforehand
  - the division of each text in the corpus has to be determined beforehand
  - based on the results, parameters can be adjusted

--

# ‚Üí

---

### Example

<br />
- corpus given:
  - document 1: goethe pushkin
  - document 2: goethe cervantes
  - document 3: pushkin cervantes
  - document 4: apple raspberry
  - document 5: apple pumpkin
  - document 6: raspberry pumpkin

<br />
<small>(This example is derived from [this video](https://www.youtube.com/watch?v=ZgyA1Q2ywbM).)</small>

--

### Example

<br />

|           | topic 1 | topic 2 |
|-----------|---------|---------|
| goethe    | 33%     | 0%      |
| pushkin   | 33%     | 0%      |
| cervantes | 33%     | 0%      |
| apple     | 0%      | 33%     |
| raspberry | 0%      | 33%     |
| pumpkin   | 0%      | 33%     |

--

### Example

<br />

|                | topic 1 | topic 2 |
|----------------|---------|---------|
| document 1     | 100%    | 0%      |
| document 2     | 100%    | 0%      |
| document 3     | 100%    | 0%      |
| document 4     | 0%      | 100%    |
| document 5     | 0%      | 100%    |
| document 6     | 0%      | 100%    |

--

### Example

<br />

- document 7:
  - "**goethe** met **pushkin** at **cervantes'** mansion, where they shared a yummy **pumpkin**."

--

### Example

<br />

|                | topic 1 | topic 2 |
|----------------|---------|---------|
| document 1     | 100%    | 0%      |
| document 2     | 100%    | 0%      |
| document 3     | 100%    | 0%      |
| document 4     | 0%      | 100%    |
| document 5     | 0%      | 100%    |
| document 6     | 0%      | 100%    |
| **document 7** | **75%** | **25%** |

--

## Pre-Processing

<br />
- **tokenizing**: breaking a text up into words, symbols, etc.
- **stemming**: reducing inflected words to their stem
- elimination of **stop words**: unlike in stylometry, frequent function words are often a hindrance in topic modeling
- **chunking**: splitting a text into smaller portions (chunks)
  - two words appearing together in a small chunk (paragraph, a scene in a play) says more about their relationship than spotting them in larger chunks (like an entire book)

--

# ‚Üí

---

### A More Realistic Example (1/2)

![example from Blei's article](images/blei_figure_1.png)<!-- .element width="860px;" -->

<small>from David M. Blei: [Probabilistic Topic Models](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf) (2012)<br />("topics and topic assignments in this figure are illustrative")</small>

--

### A More Realistic Example (2/2)

![example from Blei's article](images/blei_figure_2.png)<!-- .element width="860px;" -->

<small>from David M. Blei: [Probabilistic Topic Models](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf) (2012)<br />(100-topic LDA model; 17,000 articles from *Science*; left: topic proportions for example article in previous figure; right: top 15 MFWs from most frequent topics in this article)</small>

--

### How to Read Topics (1/2)

<br >
- example taken from ["Text Analysis with Topic Models for the Humanities and Social Sciences"](https://de.dariah.eu/tatom/index.html) by Allen Riddell:
  - six novels by Jane Austen and Charlotte Bront√´, 20&nbsp;topics

--

### How to Read Topics (2/2)

![example from Riddel's tutorial](images/austen-bronte-6-novels-20-topics.png)<!-- .element width="845px;" -->

<small>taken from Allen Riddell: [Topic modeling with MALLET](https://de.dariah.eu/tatom/topic_model_mallet.html#inspecting-the-topic-model)</small>

--

# ‚Üí

---

## Visualisation of Topic Models

<br />
- example: Stanford Literary Lab corpus (3,346 US-American, British and Irish novels from 19th century)
- examples drawn from Matthew Jockers' "Macroanalysis" (2013)
- 500 topics (overview [here](http://www.matthewjockers.net/macroanalysisbook/macro-themes/)), one of which is: [Clerks and Offices](http://www.matthewjockers.net/macroanalysisbook/macro-themes/?topic=CLERKS_AND_OFFICES)

--

## Clerks and Offices (1/3)

<br />
![Clerks and Offices, word cloud](images/clerks_and_offices_1.png)<!-- .element width="400px;" -->

<br />
<small>Cloud visualisations (source: [matthewjockers.net](http://www.matthewjockers.net/macroanalysisbook/macro-themes/?topic=CLERKS_AND_OFFICES)).</small>

--

## Clerks and Offices (2/3)

<br />
![Clerks and Offices, topics by gender](images/clerks_and_offices_2.png)<!-- .element width="400px;" -->

<br />
<small>Topics by gender (source: [matthewjockers.net](http://www.matthewjockers.net/macroanalysisbook/macro-themes/?topic=CLERKS_AND_OFFICES)).</small>

--

## Clerks and Offices (3/3)

<br />
![Clerks and Offices, topic distribution over time](images/clerks_and_offices_3.png)<!-- .element width="400px;" -->

<br />
<small>Topic distribution over time (source: [matthewjockers.net](http://www.matthewjockers.net/macroanalysisbook/macro-themes/?topic=CLERKS_AND_OFFICES)).</small>

--

### Another Example (Streamgraph):<br />Topics of a TV Show

<br />
![HIMYM topics, by Ben Schmidt](images/how_i_met_your_mother_topics_by_ben_schmidt.png)<!-- .element width="800px;" -->

<br />
<small>Ben Schmidt: [Typical TV episodes: visualizing topics in screen time](http://sappingattention.blogspot.ch/2014/12/typical-tv-episodes-visualizing-topics.html) (Dec 11, 2014)</small>

--

# ‚Üí

---

## Seminar

<br />
Hands-On with MALLET:

- "MAchine Learning for LanguagE Toolkit":
  - http://mallet.cs.umass.edu/
- follow the instructions here:
  - http://slides.com/danilsko/mallet/


</script>
			    </section>
			</div>
		</div>

		<script src="../reveal/lib/js/head.min.js"></script>
		<script src="../reveal/js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: '../reveal/plugin/markdown/marked.js' },
					{ src: '../reveal/plugin/markdown/markdown.js' },
					{ src: '../reveal/plugin/notes/notes.js', async: true },
					{ src: '../reveal/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
